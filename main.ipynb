{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cltk.corpus.utils.importer import CorpusImporter\n",
    "from cltk.corpus.readers import get_corpus_reader\n",
    "from cltk.tag.pos import POSTag\n",
    "from cltk.lemmatize.latin.backoff import BackoffLatinLemmatizer\n",
    "from cltk.tokenize.latin.sentence import SentenceTokenizer\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-a4fa31530db7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpprint\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpprint\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_importer = CorpusImporter('latin')\n",
    "\n",
    "corpus_importer.import_corpus('latin_models_cltk')\n",
    "\n",
    "corpus_importer.import_corpus('latin_text_perseus')\n",
    "\n",
    "reader = get_corpus_reader(language='latin', corpus_name='latin_text_perseus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = list(reader.docs())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perseus_sents = list(reader.sents())\n",
    "\n",
    "len(perseus_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('out.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(docs, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger = POSTag('latin')\n",
    "lemmatizer = BackoffLatinLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_tokenizer = SentenceTokenizer(strict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Gallia', None),\n",
       " ('est', 'V3SPIA---'),\n",
       " ('omnis', 'A-S---MN-'),\n",
       " ('divisa', 'T-PRPPNN-'),\n",
       " ('in', 'R--------'),\n",
       " ('partes', 'N-P---FA-'),\n",
       " ('tres', 'M--------')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger.tag_ngram_123_backoff('Gallia est omnis divisa in partes tres')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция находит в предложении предлоги, употребляющиеся с аблативом, и стоящие после них слова. Если есть что-то необычное, например, слово не в аблативе, или не имеющее падежа, то оно возвращается.\n",
    "\n",
    "1: \tpart of speech\n",
    " \n",
    " \tn\tnoun\n",
    " \tv\tverb\n",
    " \tt\tparticiple\n",
    " \ta\tadjective\n",
    " \td\tadverb\n",
    " \tc\tconjunction\n",
    " \tr\tpreposition\n",
    " \tp\tpronoun\n",
    " \tm\tnumeral\n",
    " \ti\tinterjection\n",
    " \te\texclamation\n",
    " \tu\tpunctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_analysis_to_dict(analysis, keep_empty=True):\n",
    "\n",
    "    pos = {\n",
    "        'n': 'noun',\n",
    "        'v': 'verb',\n",
    "        't': 'participle',\n",
    "        'a': 'adjective',\n",
    "        'd': 'adverb',\n",
    "        'c': 'conjunction',\n",
    "        'r': 'preposition',\n",
    "        'p': 'pronoun',\n",
    "        'm': 'numeral',\n",
    "        'i': 'interjection',\n",
    "        'e': 'exclamation',\n",
    "        'u': 'punctuation'\n",
    "    }\n",
    "    \n",
    "    person = {\n",
    "        '1': 'first person',\n",
    "        '2': 'second person',\n",
    "        '3': 'third person'\n",
    "    }\n",
    "    \n",
    "    number = {\n",
    "        's': 'singular',\n",
    "        'p': 'plural'\n",
    "    }\n",
    "    \n",
    "    tense = {\n",
    "        'p': 'present',\n",
    "        'i': 'imperfect',\n",
    "        'r': 'perfect',\n",
    "        'l': 'pluperfect',\n",
    "        't': 'future perfect',\n",
    "        'f': 'future',\n",
    "    }\n",
    "    \n",
    "    mood = {\n",
    "        'i': 'indicative',\n",
    "        's': 'subjunctive',\n",
    "        'n': 'infinitive',\n",
    "        'm': 'imperative',\n",
    "        'p': 'participle',\n",
    "        'd': 'gerund',\n",
    "        'g': 'gerundive',\n",
    "        'u': 'supine',\n",
    "    }\n",
    "    \n",
    "    voice = {\n",
    "        'a': 'active',\n",
    "        'p': 'passive',\n",
    "    }\n",
    "    \n",
    "    gender = {\n",
    "        'm': 'masculine',\n",
    "        'f': 'feminine',\n",
    "        'n': 'neuter',\n",
    "    }\n",
    "    \n",
    "    case = {\n",
    "        'n': 'nominative',\n",
    "        'g': 'genitive',\n",
    "        'd': 'dative',\n",
    "        'a': 'accusative',\n",
    "        'b': 'ablative',\n",
    "        'v': 'vocative',\n",
    "        'l': 'locative',\n",
    "    }\n",
    "    \n",
    "    degree = {\n",
    "        'c': 'comparative',\n",
    "        's': 'superlative',\n",
    "    }\n",
    "    \n",
    "    categories = {1: pos, 2: person, 3: number, 4: tense, 5:mood,\n",
    "                  6: voice, 7: gender, 8: case, 9: degree}\n",
    "    categories_names = {1: 'pos', 2: 'person', 3: 'number', 4: 'tense',\n",
    "                        5: 'mood', 6: 'voice', 7: 'gender', 8: 'case',\n",
    "                        9: 'degree'}\n",
    "    \n",
    "    \n",
    "    dict_analysis = {}\n",
    "    \n",
    "    for i, cat_value_letter in enumerate(analysis, start=1):\n",
    "        cat_name = categories_names[i]\n",
    "        if cat_value_letter == '-':\n",
    "            if keep_empty:\n",
    "                dict_analysis[cat_name] = 'N/A'\n",
    "            continue\n",
    "        try:    \n",
    "            cat_value_word = categories[i][cat_value_letter.lower()]\n",
    "            dict_analysis[cat_name] = cat_value_word\n",
    "        except KeyError as k_e:\n",
    "            print(k_e)\n",
    "            print(cat_value_letter, analysis)\n",
    "            \n",
    "    return dict_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_non_ablative_after_preposition(sent):\n",
    "    # pro вроде иногда с Acc\n",
    "    abl_preps = ('a', 'ab', 'de', 'cum', 'ex', 'e', 'sine', 'pro', 'prae')\n",
    "    results = {'sentence': None, 'strange_pairs': []}\n",
    "    \n",
    "    tagged_words =  tagger.tag_ngram_123_backoff(sent)\n",
    "    words_number = len(tagged_words)\n",
    "    \n",
    "    for i, (word, analysis) in enumerate(tagged_words):        \n",
    "        if analysis is None:# or analysis == 'None':\n",
    "            continue\n",
    "        \n",
    "        is_needed_prep = analysis[0] == 'R' and word in abl_preps\n",
    "        if not is_needed_prep:\n",
    "            continue\n",
    "        \n",
    "#         word_with_verbose_analysis = convert_analysis_to_dict(word, analysis)\n",
    "        verbose_analysis = convert_analysis_to_dict(analysis)\n",
    "        verbose_analysis['word'] = word\n",
    "        word_lemma = lemmatizer.lemmatize([word])[0][1]\n",
    "        verbose_analysis['lemma'] = word_lemma\n",
    "\n",
    "        is_last = i == words_number - 1        \n",
    "        if is_last:\n",
    "            results['sentence'] = sent\n",
    "            results['strange_pairs'].append(\n",
    "                (verbose_analysis, ('%END%', '%END')))\n",
    "        else:\n",
    "            next_word, next_word_analysis = tagged_words[i+1]\n",
    "            if next_word_analysis is None:\n",
    "                continue\n",
    "                \n",
    "            if next_word_analysis[8-1].lower() != 'b':\n",
    "                next_word_verbose_analysis = convert_analysis_to_dict(\n",
    "                    next_word_analysis)\n",
    "                next_word_verbose_analysis['word'] = next_word\n",
    "                next_word_lemma = lemmatizer.lemmatize([next_word])[0][1]\n",
    "                next_word_verbose_analysis['lemma'] = next_word_lemma\n",
    "                \n",
    "                results['sentence'] = sent\n",
    "                results['strange_pairs'].append((verbose_analysis,\n",
    "                                                next_word_verbose_analysis))\n",
    "            \n",
    "    if results['sentence'] is None:\n",
    "        return None\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_dict_into_str(iter_):\n",
    "    if isinstance(iter_, str):\n",
    "        yield iter_\n",
    "    else:\n",
    "        try:\n",
    "            for i, obj in iter_.items():\n",
    "                yield from flatten_dict_into_str(obj)\n",
    "        except:\n",
    "            try:\n",
    "                for obj in iter_:\n",
    "                    yield from flatten_dict_into_str(obj)\n",
    "            except:\n",
    "                yield iter_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_document(doc):\n",
    "    analysis_for_doc = {'author': doc['author'], 'title': doc['originalTitle'],\n",
    "                       'sentences': []}\n",
    "    \n",
    "    text = ' '.join(map(str, flatten_dict_into_str(doc['text'])))\n",
    "    \n",
    "    sentences = sent_tokenizer.tokenize(text)\n",
    "    for sent in tqdm(sentences):\n",
    "        sent_data = get_non_ablative_after_preposition(sent)\n",
    "        if sent_data is None:\n",
    "            continue\n",
    "            \n",
    "        analysis_for_doc['sentences'].append(sent_data)\n",
    "    \n",
    "    return analysis_for_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66f8cff453304166ab79fa09936c670b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7c2ad3701a34a659d76b431f0dfc7b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5006.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6194860054cd425aaf4c5d131ea6665e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1315.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'1'\n",
      "1 V1SPIA---\n",
      "'3'\n",
      "3 V3SPSA---\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# count = 3000\n",
    "\n",
    "# results = []\n",
    "\n",
    "\n",
    "# for sent in tqdm(perseus_sents[:count]):\n",
    "#     result = get_non_ablative_after_preposition(sent)\n",
    "#     if result is not None:\n",
    "#         results.append(result)\n",
    "\n",
    "count_doc = 2\n",
    "\n",
    "results = []\n",
    "\n",
    "for doc in tqdm(docs[:count_doc]):\n",
    "    result = analyse_document(doc)\n",
    "    if result is not None:\n",
    "        results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('res_2docs_perseus.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(results, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "184"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(results[0]['sentences'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сделать из списка словарей, каждый из которых описывает один документ, датафрейм. Каждой паре слов из словаря присвоить метаданные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_dict_to_dataframe(results):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs[0].sents()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-",
   "formats": "ipynb,py:hydrogen"
  },
  "kernelspec": {
   "display_name": "Python (cltk)",
   "language": "python",
   "name": "cltk_project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
