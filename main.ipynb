{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cltk.corpus.utils.importer import CorpusImporter\n",
    "from cltk.corpus.readers import get_corpus_reader\n",
    "from cltk.tag.pos import POSTag\n",
    "from cltk.lemmatize.latin.backoff import BackoffLatinLemmatizer\n",
    "from cltk.tokenize.latin.sentence import SentenceTokenizer\n",
    "from tqdm.auto import tqdm\n",
    "from pandas.api.types import CategoricalDtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-49-51d6c7156141>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from datetime import datetime\n",
    "from itertools import zip_longest, islice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cProfile\n",
    "# import pstats\n",
    "# from pstats import SortKey\n",
    "# import tracemalloc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_rows\", 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_importer = CorpusImporter('latin')\n",
    "\n",
    "corpus_importer.import_corpus('latin_models_cltk')\n",
    "corpus_importer.import_corpus('latin_text_perseus')\n",
    "\n",
    "reader = get_corpus_reader(language='latin', corpus_name='latin_text_perseus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perseus_sents = list(reader.sents())\n",
    "\n",
    "# len(perseus_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('out.json', 'w', encoding='utf-8') as f:\n",
    "#     json.dump(docs, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger = POSTag('latin')\n",
    "lemmatizer = BackoffLatinLemmatizer()\n",
    "\n",
    "sent_tokenizer = SentenceTokenizer(strict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# def get_categories():\n",
    "pos = {\n",
    "    'n': 'noun',\n",
    "    'v': 'verb',\n",
    "    't': 'participle',\n",
    "    'a': 'adjective',\n",
    "    'd': 'adverb',\n",
    "    'c': 'conjunction',\n",
    "    'r': 'preposition',\n",
    "    'p': 'pronoun',\n",
    "    'm': 'numeral',\n",
    "    'i': 'interjection',\n",
    "    'e': 'exclamation',\n",
    "    'u': 'punctuation'\n",
    "}\n",
    "\n",
    "person = {\n",
    "    '1': 'first person',\n",
    "    '2': 'second person',\n",
    "    '3': 'third person'\n",
    "}\n",
    "\n",
    "number = {\n",
    "    's': 'singular',\n",
    "    'p': 'plural'\n",
    "}\n",
    "\n",
    "tense = {\n",
    "    'p': 'present',\n",
    "    'i': 'imperfect',\n",
    "    'r': 'perfect',\n",
    "    'l': 'pluperfect',\n",
    "    't': 'future perfect',\n",
    "    'f': 'future',\n",
    "}\n",
    "\n",
    "mood = {\n",
    "    'i': 'indicative',\n",
    "    's': 'subjunctive',\n",
    "    'n': 'infinitive',\n",
    "    'm': 'imperative',\n",
    "    'p': 'participle',\n",
    "    'd': 'gerund',\n",
    "    'g': 'gerundive',\n",
    "    'u': 'supine',\n",
    "}\n",
    "\n",
    "voice = {\n",
    "    'a': 'active',\n",
    "    'p': 'passive',\n",
    "}\n",
    "\n",
    "gender = {\n",
    "    'm': 'masculine',\n",
    "    'f': 'feminine',\n",
    "    'n': 'neuter',\n",
    "}\n",
    "\n",
    "case = {\n",
    "    'n': 'nominative',\n",
    "    'g': 'genitive',\n",
    "    'd': 'dative',\n",
    "    'a': 'accusative',\n",
    "    'b': 'ablative',\n",
    "    'v': 'vocative',\n",
    "    'l': 'locative',\n",
    "}\n",
    "\n",
    "degree = {\n",
    "    'c': 'comparative',\n",
    "    's': 'superlative',\n",
    "}\n",
    "\n",
    "categories = {'pos': pos, 'person': person, 'number': number, 'tense': tense,\n",
    "              'mood': mood, 'voice': voice, 'gender': gender, 'case': case,\n",
    "              'degree': degree}\n",
    "categories_names = {1: 'pos', 2: 'person', 3: 'number', 4: 'tense',\n",
    "                    5: 'mood', 6: 'voice', 7: 'gender', 8: 'case',\n",
    "                    9: 'degree'}\n",
    "    \n",
    "#     return categories, categories_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_analysis_to_dict(analysis, keep_empty=True):   \n",
    "    dict_analysis = {}\n",
    "    \n",
    "    for i, cat_value_letter in enumerate(analysis, start=1):\n",
    "        cat_name = categories_names[i]\n",
    "        if cat_value_letter == '-':\n",
    "            if keep_empty:\n",
    "                dict_analysis[cat_name] = 'N/A'\n",
    "            continue\n",
    "        try:    \n",
    "            cat_value_word = categories[cat_name][cat_value_letter.lower()]\n",
    "            dict_analysis[cat_name] = cat_value_word\n",
    "        except KeyError as k_e:\n",
    "            print(k_e)\n",
    "            print(cat_value_letter, analysis)\n",
    "            \n",
    "    return dict_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_foreign_lang_in_sent(sent):\n",
    "    # истинно для открывков вроде\n",
    "    # h(\\ to/sa fa/rmaka h)/dh, o(/sa tre/fei eu)rei=a xqw/n\n",
    "    foreign_chars = set(['\\\\', '/', '|', '='])\n",
    "#     foreign_chars_code = re.compile(r'\\\\\\w+|\\w\\\\|/\\w|\\w/|\\||=')\n",
    "#     return bool(re.search(foreign_chars_code, sent))\n",
    "    return any(word in sent for word in foreign_chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция находит в предложении предлоги, употребляющиеся с аблативом, и стоящие после них слова. Если есть что-то необычное, например, слово не в аблативе, или не имеющее падежа, то оно возвращается.\n",
    "\n",
    "Сперва сделаем разборы слов предложения, заодно они токенизируются. Все слова на время сохраним в `words_in_sent`. Если среди них нет ни одного нужного предлога, функция сразу вернёт `None`. Если предлоги есть, то идёт проход по всем словам, проверка каждого слова и если это нужный предлог, то проверяется следующее слово. Если предлог - последнее слово в предложении (вернее в клаузе), то результат обозначается специальным маркером `%END%`, в противном случае анализируется следующее слово."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_non_ablative_after_preposition(sent):   \n",
    "    \n",
    "    # pro вроде иногда с Acc\n",
    "    abl_preps = set(('a', 'ab', 'de', 'cum', 'ex', 'e', 'sine', 'pro', 'prae'))\n",
    "    results = {'sentence': None, 'strange_pairs': []}\n",
    "    \n",
    "    tagged_words =  tagger.tag_ngram_123_backoff(sent)\n",
    "    words_number = len(tagged_words)\n",
    "    \n",
    "    words_in_sent, _ = zip(*tagged_words)\n",
    "    if not any(prep in words_in_sent for prep in abl_preps):\n",
    "        return None\n",
    "    \n",
    "    for i, (word, analysis) in enumerate(tagged_words):        \n",
    "        if analysis is None:\n",
    "            continue\n",
    "        \n",
    "        is_needed_prep = analysis[0] == 'R' and word in abl_preps\n",
    "        if not is_needed_prep:\n",
    "            continue\n",
    "        \n",
    "        verbose_analysis = convert_analysis_to_dict(analysis)\n",
    "        verbose_analysis['word'] = word\n",
    "        [(_, word_lemma)] = lemmatizer.lemmatize([word])\n",
    "        verbose_analysis['lemma'] = word_lemma\n",
    "\n",
    "        is_last = i == words_number - 1        \n",
    "        if is_last:\n",
    "            results['sentence'] = sent\n",
    "            results['strange_pairs'].append(\n",
    "                (verbose_analysis,\n",
    "                 {cat_name: '%END%' for cat_name in list(categories.keys())+['word', 'lemma']}))\n",
    "        else:\n",
    "            next_word, next_word_analysis = tagged_words[i+1]\n",
    "            if next_word_analysis is None:\n",
    "                # может всё-таки брать такие слова?\n",
    "                continue\n",
    "                \n",
    "            if next_word_analysis[8-1].lower() != 'b':\n",
    "                next_word_verbose_analysis = convert_analysis_to_dict(\n",
    "                    next_word_analysis)\n",
    "                next_word_verbose_analysis['word'] = next_word\n",
    "                [(_, next_word_lemma)] = lemmatizer.lemmatize([next_word])\n",
    "                next_word_verbose_analysis['lemma'] = next_word_lemma\n",
    "                \n",
    "                results['sentence'] = sent\n",
    "                results['strange_pairs'].append((verbose_analysis,\n",
    "                                                next_word_verbose_analysis))\n",
    "            \n",
    "    if results['sentence'] is None:\n",
    "        return None\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_dict_into_str(iter_):\n",
    "    if isinstance(iter_, str):\n",
    "        yield iter_\n",
    "    else:\n",
    "        try:\n",
    "            for i, obj in iter_.items():\n",
    "                yield from flatten_dict_into_str(obj)\n",
    "        except:\n",
    "            try:\n",
    "                for obj in iter_:\n",
    "                    yield from flatten_dict_into_str(obj)\n",
    "            except:\n",
    "                yield iter_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция `grouper()` из [itertools recipes](https://docs.python.org/library/itertools.html#itertools-recipes)\n",
    "\n",
    "> this is feeding the same iterator to `izip_longest` multiple times, causing it to consume successive values of the same sequence rather than striped values from separate sequences. [StackOverflow](https://stackoverflow.com/questions/434287/what-is-the-most-pythonic-way-to-iterate-over-a-list-in-chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grouper(iterable, chunk_size, fillvalue=None):\n",
    "    args = [iter(iterable)] * chunk_size\n",
    "    return zip_longest(*args, fillvalue=fillvalue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_document(doc):\n",
    "    analysis_for_doc = {'author': doc['author'], 'title': doc['originalTitle'],\n",
    "                       'sentences': []}\n",
    "    \n",
    "    text = ' '.join([str(el) for el in flatten_dict_into_str(doc['text'])])\n",
    "    sentences = sent_tokenizer.tokenize(text)\n",
    "    \n",
    "    ### chunking\n",
    "    num_of_sent = len(sentences)\n",
    "    chunk_size = 300\n",
    "    if num_of_sent >= 2 * chunk_size:\n",
    "        sentences = grouper(sentences, chunk_size)\n",
    "    else:\n",
    "        sentences = [sentences]\n",
    "        \n",
    "    tqdm.write(str(type(sentences)))\n",
    "    \n",
    "    for sent_chunk in tqdm(sentences):\n",
    "        for sent in sent_chunk:\n",
    "            # если в предложении вкрапления греческого, которые в корпусе \n",
    "            # записаны очень странно, то пропускаем это предложение\n",
    "            # также пропускаем пустое предложение, которое могло появиться из-за\n",
    "            # fillvalue в итераторе \n",
    "            if sent is None or is_foreign_lang_in_sent(sent):\n",
    "                continue\n",
    "\n",
    "            sent_data = get_non_ablative_after_preposition(sent)\n",
    "            if sent_data is None:\n",
    "                continue\n",
    "\n",
    "            analysis_for_doc['sentences'].append(sent_data)\n",
    "    \n",
    "    if not analysis_for_doc['sentences']:\n",
    "        return None\n",
    "    \n",
    "    return analysis_for_doc\n",
    "\n",
    "    \n",
    "#     for sent in tqdm(sentences):\n",
    "#         # если в предложении вкрапления греческого, которые в корпусе \n",
    "#         # записаны очень странно, то пропускаем это предложение \n",
    "#         if is_foreign_lang_in_sent(sent):\n",
    "#             continue\n",
    "        \n",
    "#         sent_data = get_non_ablative_after_preposition(sent)\n",
    "#         if sent_data is None:\n",
    "#             continue\n",
    "   \n",
    "#         analysis_for_doc['sentences'].append(sent_data)\n",
    "    \n",
    "#     return analysis_for_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выше в качестве `docs` взят генератор. Это довольно быстро по времени, и экономнее по памяти. **NB**: характерные гребни на графике использования RAM когда выполняется цикл."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_from_x_docs(count_doc, start=0):\n",
    "    # count = 3000\n",
    "    # results = []\n",
    "    # for sent in tqdm(perseus_sents[:count]):\n",
    "    #     result = get_non_ablative_after_preposition(sent)\n",
    "    #     if result is not None:\n",
    "    #         results.append(result)\n",
    "\n",
    "    results = []\n",
    "\n",
    "#     for doc in tqdm(docs[:count_doc]):\n",
    "    for doc in tqdm(islice(docs, start, count_doc)):\n",
    "        result = analyse_document(doc)\n",
    "        if result is not None:\n",
    "            results.append(result)\n",
    "            \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = reader.docs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f0396d297034137a9d944d0a3ea07c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'itertools.zip_longest'>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc2e0b64681049edb235f22de997f80e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<class 'itertools.zip_longest'>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac6948faa6d44ecda84c0d435ae867ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<class 'list'>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8901f9d389a04d2db75ae160bd8b39e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<class 'itertools.zip_longest'>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb97f5dd6d5b4e25999226f1724a283e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<class 'list'>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e0a7932e8274337984b25dbb3db1cb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<class 'list'>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54a922bd35fe4a58b6bf65f59a151515",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<class 'list'>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02348ecb6c3342c19881403642acb836",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<class 'list'>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e11f87ceb9943cebdc1e73993c5a1c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<class 'list'>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81520ce7987444f89ca43e58e49e8e01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<class 'list'>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0aa7754b72694f8aa536b06db0ef437b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# tracemalloc.start()\n",
    "count_doc = 10\n",
    "\n",
    "# time = datetime.now().strftime('%H.%M.%S-%d-%m')\n",
    "# STATS_FILENAME = 'runstats.'.format(time)\n",
    "# cProfile.run('get_data_from_x_docs(count_doc)', STATS_FILENAME) \n",
    "# results = get_data_from_x_docs(count_doc)\n",
    "\n",
    "results = get_data_from_x_docs(count_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top_stats = snapshot.statistics('lineno')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"[ Top 10 ]\")\n",
    "# for stat in top_stats[:20]:\n",
    "#     print(stat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p = pstats.Stats('runstats')\n",
    "# p.strip_dirs().sort_stats(SortKey.CUMULATIVE).print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('res_2docs_perseus.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(results, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(results[6]['sentences'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сделать из списка словарей, каждый из которых описывает один документ, датафрейм. Каждой паре слов из словаря присвоить метаданные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_dict_to_dataframe(results):\n",
    "    author = results['author']\n",
    "    title = results['title']\n",
    "    entries_list = []\n",
    "    \n",
    "    for sentence_and_word_pairs_dict in results['sentences']:\n",
    "        sentence = sentence_and_word_pairs_dict['sentence'].replace('\\n', ' ')\n",
    "        word_pairs = sentence_and_word_pairs_dict['strange_pairs']\n",
    "        for prep_dict, second_word_dict in word_pairs:\n",
    "            entry = {'prep': prep_dict['word'], **second_word_dict,\n",
    "                     'sentence': sentence,\n",
    "                     'author': author, 'title': title}\n",
    "            entries_list.append(entry)\n",
    "    \n",
    "    \n",
    "    df = pd.DataFrame(entries_list)\n",
    "    cols = ['prep', 'word', 'lemma', 'pos', 'person', 'number', 'tense', 'mood',\n",
    "            'voice', 'gender', 'case', 'degree', 'sentence', 'author', 'title']\n",
    "    df = df[cols]  \n",
    "    \n",
    "    df.replace('N/A', np.nan, inplace=True)\n",
    "    \n",
    "    dtypes = {cat_name: CategoricalDtype(categories=cat_map.values())\n",
    "              for cat_name, cat_map in categories.items()}\n",
    "    dtypes['prep'] = 'category'\n",
    "    \n",
    "    df_new = df.astype(dtypes)\n",
    "    \n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full = pd.concat(list(map(convert_dict_to_dataframe, results)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_prep = df.groupby('prep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_pos = df[['pos', 'word', 'lemma', 'sentence']].groupby(['pos'])['word'].count()\n",
    "by_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['pos', 'word', 'lemma', 'sentence']].groupby(['pos', 'word'])['word'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prep</th>\n",
       "      <th>word</th>\n",
       "      <th>lemma</th>\n",
       "      <th>pos</th>\n",
       "      <th>person</th>\n",
       "      <th>number</th>\n",
       "      <th>tense</th>\n",
       "      <th>mood</th>\n",
       "      <th>voice</th>\n",
       "      <th>gender</th>\n",
       "      <th>case</th>\n",
       "      <th>degree</th>\n",
       "      <th>sentence</th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>409</td>\n",
       "      <td>409</td>\n",
       "      <td>409</td>\n",
       "      <td>403</td>\n",
       "      <td>6</td>\n",
       "      <td>339</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>333</td>\n",
       "      <td>332</td>\n",
       "      <td>0</td>\n",
       "      <td>409</td>\n",
       "      <td>409</td>\n",
       "      <td>409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>9</td>\n",
       "      <td>190</td>\n",
       "      <td>164</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>394</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>a</td>\n",
       "      <td>me</td>\n",
       "      <td>ego</td>\n",
       "      <td>noun</td>\n",
       "      <td>third person</td>\n",
       "      <td>singular</td>\n",
       "      <td>perfect</td>\n",
       "      <td>participle</td>\n",
       "      <td>passive</td>\n",
       "      <td>masculine</td>\n",
       "      <td>accusative</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ergo postquam factus est imperator Zeno a fili...</td>\n",
       "      <td>apuleius</td>\n",
       "      <td>Rerum Gestarum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>86</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>136</td>\n",
       "      <td>4</td>\n",
       "      <td>264</td>\n",
       "      <td>12</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>149</td>\n",
       "      <td>99</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>213</td>\n",
       "      <td>189</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       prep word lemma   pos        person    number    tense        mood  \\\n",
       "count   409  409   409   403             6       339       20          20   \n",
       "unique    9  190   164    11             2         2        2           5   \n",
       "top       a   me   ego  noun  third person  singular  perfect  participle   \n",
       "freq     86   40    40   136             4       264       12          11   \n",
       "\n",
       "          voice     gender        case degree  \\\n",
       "count        20        333         332      0   \n",
       "unique        2          3           5      0   \n",
       "top     passive  masculine  accusative    NaN   \n",
       "freq         10        149          99    NaN   \n",
       "\n",
       "                                                 sentence    author  \\\n",
       "count                                                 409       409   \n",
       "unique                                                394         3   \n",
       "top     Ergo postquam factus est imperator Zeno a fili...  apuleius   \n",
       "freq                                                    3       213   \n",
       "\n",
       "                 title  \n",
       "count              409  \n",
       "unique               7  \n",
       "top     Rerum Gestarum  \n",
       "freq               189  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_full.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pos          word         \n",
       "noun         adulescentiae    1\n",
       "             aetatis          2\n",
       "             amico            1\n",
       "             amnis            1\n",
       "             anni             1\n",
       "                             ..\n",
       "numeral      septem           2\n",
       "             tribus           3\n",
       "exclamation  ma               1\n",
       "punctuation  ,                2\n",
       "             -                1\n",
       "Name: word, Length: 186, dtype: int64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_full[['pos', 'word', 'lemma', 'sentence']].groupby(['pos', 'word'])['word'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "prep  pos        \n",
       "a     noun           24\n",
       "      participle      8\n",
       "      adjective       7\n",
       "      adverb          5\n",
       "      conjunction     3\n",
       "      pronoun        34\n",
       "      numeral         1\n",
       "ab    noun           11\n",
       "      participle      1\n",
       "      adjective       1\n",
       "      adverb          2\n",
       "      pronoun        10\n",
       "cum   noun           12\n",
       "      verb            4\n",
       "      adjective       6\n",
       "      adverb          9\n",
       "      preposition     8\n",
       "      pronoun        11\n",
       "      numeral         1\n",
       "      punctuation     1\n",
       "de    noun           27\n",
       "      verb            3\n",
       "      participle      1\n",
       "      adjective      24\n",
       "      adverb          5\n",
       "      conjunction     1\n",
       "      pronoun        24\n",
       "      punctuation     1\n",
       "e     noun            3\n",
       "      adverb          1\n",
       "      preposition     1\n",
       "      pronoun         1\n",
       "      numeral         3\n",
       "ex    noun           18\n",
       "      participle      2\n",
       "      adjective      18\n",
       "      adverb          2\n",
       "      preposition     1\n",
       "      pronoun        13\n",
       "      numeral         1\n",
       "prae  noun            1\n",
       "      pronoun        11\n",
       "      punctuation     1\n",
       "pro   noun           32\n",
       "      adjective       1\n",
       "      adverb          2\n",
       "      pronoun         8\n",
       "      numeral         1\n",
       "      exclamation     1\n",
       "sine  noun            8\n",
       "      verb            1\n",
       "      adjective      21\n",
       "      adverb          5\n",
       "      pronoun         1\n",
       "Name: word, dtype: int64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_full[['prep', 'pos', 'word']].groupby(['prep', 'pos'])['word'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prep</th>\n",
       "      <th>word</th>\n",
       "      <th>lemma</th>\n",
       "      <th>pos</th>\n",
       "      <th>person</th>\n",
       "      <th>number</th>\n",
       "      <th>tense</th>\n",
       "      <th>mood</th>\n",
       "      <th>voice</th>\n",
       "      <th>gender</th>\n",
       "      <th>case</th>\n",
       "      <th>degree</th>\n",
       "      <th>sentence</th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [prep, word, lemma, pos, person, number, tense, mood, voice, gender, case, degree, sentence, author, title]\n",
       "Index: []"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_full[df_full['word'] == '%END%']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-",
   "formats": "ipynb,py:hydrogen"
  },
  "kernelspec": {
   "display_name": "Python (cltk)",
   "language": "python",
   "name": "cltk_project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
