{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Что бывает после предлогов, управляющих аблативом (кроме аблатива)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Библиотека cltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cltk.corpus.utils.importer import CorpusImporter\n",
    "from cltk.corpus.readers import get_corpus_reader\n",
    "from cltk.tag.pos import POSTag\n",
    "from cltk.lemmatize.latin.backoff import BackoffLatinLemmatizer\n",
    "from cltk.tokenize.latin.sentence import SentenceTokenizer\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.api.types import CategoricalDtype\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "from datetime import datetime\n",
    "from itertools import zip_longest, islice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Графики"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg' \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Библиотеки для отслеживания памяти и измерения времени, в итоговом билде они не нужны"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cProfile\n",
    "# import pstats\n",
    "# from pstats import SortKey\n",
    "# import sys\n",
    "# import os\n",
    "# os.environ[\"PYTHONTRACEMALLOC\"] = '10'\n",
    "# import tracemalloc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tracemalloc.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_rows\", 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Корпус perseus из cltk и модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_importer = CorpusImporter('latin')\n",
    "\n",
    "corpus_importer.import_corpus('latin_models_cltk')\n",
    "corpus_importer.import_corpus('latin_text_perseus')\n",
    "\n",
    "reader = get_corpus_reader(language='latin', corpus_name='latin_text_perseus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger = POSTag('latin')\n",
    "lemmatizer = BackoffLatinLemmatizer()\n",
    "sent_tokenizer = SentenceTokenizer(strict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Основная часть проекта"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Соответствие кратких тегов нормальным словам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_categories():\n",
    "pos = {\n",
    "    'n': 'noun',\n",
    "    'v': 'verb',\n",
    "    't': 'participle',\n",
    "    'a': 'adjective',\n",
    "    'd': 'adverb',\n",
    "    'c': 'conjunction',\n",
    "    'r': 'preposition',\n",
    "    'p': 'pronoun',\n",
    "    'm': 'numeral',\n",
    "    'i': 'interjection',\n",
    "    'e': 'exclamation',\n",
    "    'u': 'punctuation'\n",
    "}\n",
    "\n",
    "person = {\n",
    "    '1': 'first person',\n",
    "    '2': 'second person',\n",
    "    '3': 'third person'\n",
    "}\n",
    "\n",
    "number = {\n",
    "    's': 'singular',\n",
    "    'p': 'plural'\n",
    "}\n",
    "\n",
    "tense = {\n",
    "    'p': 'present',\n",
    "    'i': 'imperfect',\n",
    "    'r': 'perfect',\n",
    "    'l': 'pluperfect',\n",
    "    't': 'future perfect',\n",
    "    'f': 'future',\n",
    "}\n",
    "\n",
    "mood = {\n",
    "    'i': 'indicative',\n",
    "    's': 'subjunctive',\n",
    "    'n': 'infinitive',\n",
    "    'm': 'imperative',\n",
    "    'p': 'participle',\n",
    "    'd': 'gerund',\n",
    "    'g': 'gerundive',\n",
    "    'u': 'supine',\n",
    "}\n",
    "\n",
    "voice = {\n",
    "    'a': 'active',\n",
    "    'p': 'passive',\n",
    "}\n",
    "\n",
    "gender = {\n",
    "    'm': 'masculine',\n",
    "    'f': 'feminine',\n",
    "    'n': 'neuter',\n",
    "}\n",
    "\n",
    "case = {\n",
    "    'n': 'nominative',\n",
    "    'g': 'genitive',\n",
    "    'd': 'dative',\n",
    "    'a': 'accusative',\n",
    "    'b': 'ablative',\n",
    "    'v': 'vocative',\n",
    "    'l': 'locative',\n",
    "}\n",
    "\n",
    "degree = {\n",
    "    'c': 'comparative',\n",
    "    's': 'superlative',\n",
    "}\n",
    "\n",
    "categories = {'pos': pos, 'person': person, 'number': number, 'tense': tense,\n",
    "              'mood': mood, 'voice': voice, 'gender': gender, 'case': case,\n",
    "              'degree': degree}\n",
    "categories_names = {1: 'pos', 2: 'person', 3: 'number', 4: 'tense',\n",
    "                    5: 'mood', 6: 'voice', 7: 'gender', 8: 'case',\n",
    "                    9: 'degree'}\n",
    "    \n",
    "#     return categories, categories_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция ```convert_analysis_to_dict()``` конвертирует 9 символьную строку анализа, в словарь \"категория - значение\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_analysis_to_dict(analysis, keep_empty=True):   \n",
    "    dict_analysis = {}\n",
    "    \n",
    "    for i, cat_value_letter in enumerate(analysis, start=1):\n",
    "        cat_name = categories_names[i]\n",
    "        if cat_value_letter == '-':\n",
    "            if keep_empty:\n",
    "                dict_analysis[cat_name] = 'N/A'\n",
    "            continue\n",
    "        try:    \n",
    "            cat_value_word = categories[cat_name][cat_value_letter.lower()]\n",
    "            dict_analysis[cat_name] = cat_value_word\n",
    "        except KeyError as k_e:\n",
    "            print(k_e)\n",
    "            print(cat_value_letter, analysis)\n",
    "            \n",
    "    return dict_analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция ```is_foreign_lang_in_sent()``` возвращает ```False```, если в предложении есть хотя бы один странный символ. Они используются в корпусе для кодирования иностранных языков внутри текста."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_foreign_lang_in_sent(sent):\n",
    "    # истинно для открывков вроде\n",
    "    # h(\\ to/sa fa/rmaka h)/dh, o(/sa tre/fei eu)rei=a xqw/n\n",
    "    foreign_chars = set(['\\\\', '/', '|', '='])\n",
    "    return any(word in sent for word in foreign_chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция ```get_non_ablative_after_preposition()``` находит в предложении предлоги, употребляющиеся с аблативом, и стоящие после них слова. Если есть что-то необычное, например, слово не в аблативе, или не имеющее падежа, то оно возвращается.\n",
    "\n",
    "Сперва сделаем разборы слов предложения, заодно они токенизируются. Все слова на время сохраним в `words_in_sent`. Если среди них нет ни одного нужного предлога, функция сразу вернёт `None`. Если предлоги есть, то идёт проход по всем словам, проверка каждого слова и если это нужный предлог, то проверяется следующее слово. Если предлог - последнее слово в предложении (вернее в клаузе), то результат обозначается специальным маркером `%END%`, в противном случае анализируется следующее слово."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_non_ablative_after_preposition(sent, context_size=8):   \n",
    "    \n",
    "    abl_preps = set(('a', 'ab', 'de', 'cum', 'ex', 'e', 'sine', 'pro', 'prae'))\n",
    "    results = {'sentence': None, 'strange_pairs': [], 'contexts': []}\n",
    "    \n",
    "    tagged_words =  tagger.tag_ngram_123_backoff(sent)\n",
    "    words_number = len(tagged_words)\n",
    "    \n",
    "    words_in_sent, _ = zip(*tagged_words)\n",
    "    if not any(prep in words_in_sent for prep in abl_preps):\n",
    "        return None\n",
    "    \n",
    "    for i, (word, analysis) in enumerate(tagged_words):        \n",
    "        if analysis is None:\n",
    "            continue\n",
    "        \n",
    "        is_needed_prep = analysis[0] == 'R' and word in abl_preps\n",
    "        if not is_needed_prep:\n",
    "            continue\n",
    "        \n",
    "        verbose_analysis = convert_analysis_to_dict(analysis)\n",
    "        verbose_analysis['word'] = word\n",
    "        [(_, word_lemma)] = lemmatizer.lemmatize([word])\n",
    "        verbose_analysis['lemma'] = word_lemma\n",
    "\n",
    "        is_last = i == words_number - 1        \n",
    "        if is_last:\n",
    "            results['sentence'] = sent\n",
    "            results['strange_pairs'].append(\n",
    "                (verbose_analysis,\n",
    "                 {cat_name: '%END%' for cat_name in list(categories.keys())+['word', 'lemma']}))\n",
    "            results['contexts'].append(words_in_sent[max(i-context_size, 0):])\n",
    "            \n",
    "        else:\n",
    "            next_word, next_word_analysis = tagged_words[i+1]\n",
    "            if next_word_analysis is None:\n",
    "                # может всё-таки брать такие слова?\n",
    "                continue\n",
    "                \n",
    "            if next_word_analysis[8-1].lower() != 'b':\n",
    "                next_word_verbose_analysis = convert_analysis_to_dict(\n",
    "                    next_word_analysis)\n",
    "                next_word_verbose_analysis['word'] = next_word\n",
    "                [(_, next_word_lemma)] = lemmatizer.lemmatize([next_word])\n",
    "                next_word_verbose_analysis['lemma'] = next_word_lemma\n",
    "                \n",
    "                results['sentence'] = sent\n",
    "                results['strange_pairs'].append((verbose_analysis,\n",
    "                                                next_word_verbose_analysis))\n",
    "                results['contexts'].append(words_in_sent[max(i-context_size, 0):min(i+context_size, words_number)])\n",
    "            \n",
    "    if results['sentence'] is None:\n",
    "        return None\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция для обработки одного документа. Достаёт весь текст, соединённый в строку. (Точнее это генератор, возвращающий по одном куску текста)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_dict_into_str(iter_):\n",
    "    if isinstance(iter_, str):\n",
    "        yield iter_\n",
    "    else:\n",
    "        try:\n",
    "            for i, obj in iter_.items():\n",
    "                yield from flatten_dict_into_str(obj)\n",
    "        except:\n",
    "            try:\n",
    "                for obj in iter_:\n",
    "                    yield from flatten_dict_into_str(obj)\n",
    "            except:\n",
    "                yield iter_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция `grouper()` из [itertools recipes](https://docs.python.org/library/itertools.html#itertools-recipes)\n",
    "\n",
    "> this is feeding the same iterator to `izip_longest` multiple times, causing it to consume successive values of the same sequence rather than striped values from separate sequences. [StackOverflow](https://stackoverflow.com/questions/434287/what-is-the-most-pythonic-way-to-iterate-over-a-list-in-chunks)\n",
    "\n",
    "Ещё вариант:\n",
    "\n",
    "```python\n",
    "def chunker(seq, size):\n",
    "    return (seq[pos:pos + size] for pos in range(0, > len(seq), size))\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grouper(iterable, chunk_size, fillvalue=None):\n",
    "    args = [iter(iterable)] * chunk_size\n",
    "    return zip_longest(*args, fillvalue=fillvalue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция ```analyse_document()``` склеивает все предложения в документе в единый текст, а потом разбивает его по предложениям, анализируя предложения группами, в зависимости от их количества в документе. Если \"предложение\" это ```None``` или в нём есть вкрапления на другом языке, то предложение пропускается. Если же всё хорошо, то к предложению применяется описанная выше ```get_non_ablative_after_preposition()```. Если всё совсем хорошо и функция вернула непустое значение, то оно записывается в словарь данных по документу."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_document(doc):\n",
    "    analysis_for_doc = {'author': doc['author'], 'title': doc['originalTitle'],\n",
    "                       'sentences': []}\n",
    "    \n",
    "    text = ' '.join([str(el) for el in flatten_dict_into_str(doc['text'])])\n",
    "    sentences = sent_tokenizer.tokenize(text)\n",
    "    \n",
    "    ### chunking\n",
    "    num_of_sent = len(sentences)\n",
    "    chunk_size = 300\n",
    "    if num_of_sent >= 2 * chunk_size:\n",
    "        sentences = grouper(sentences, chunk_size)\n",
    "    else:\n",
    "        sentences = [sentences]\n",
    "    \n",
    "    tqdm.write('Now analyzing ' + doc['author'] + ' ' + doc['originalTitle'])\n",
    "    tqdm.write('Num of sentences: ' + str(num_of_sent))\n",
    "    tqdm.write(str(type(sentences)))\n",
    "    \n",
    "    for sent_chunk in tqdm(sentences):\n",
    "        for sent in sent_chunk:\n",
    "            # если в предложении вкрапления греческого, которые в корпусе \n",
    "            # записаны очень странно, то пропускаем это предложение\n",
    "            # также пропускаем пустое предложение, которое могло появиться из-за\n",
    "            # fillvalue в итераторе \n",
    "            if sent is None or is_foreign_lang_in_sent(sent):\n",
    "                continue\n",
    "\n",
    "            sent_data = get_non_ablative_after_preposition(sent)\n",
    "            if sent_data is None:\n",
    "                continue\n",
    "\n",
    "            analysis_for_doc['sentences'].append(sent_data)\n",
    "    \n",
    "    if not analysis_for_doc['sentences']:\n",
    "        return None\n",
    "    \n",
    "    return analysis_for_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция ```get_data_from_x_docs()``` возвращает все данные для нескольких документов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_from_x_docs(docs, count_doc, start=0):\n",
    "    results = []\n",
    "\n",
    "    for doc in tqdm(islice(docs, start, count_doc)):\n",
    "        result = analyse_document(doc)\n",
    "        if result is not None:\n",
    "            results.append(result)\n",
    "            \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве `docs` взят генератор. Это довольно быстро по времени, и экономнее по памяти. **NB**: характерные гребни на графике использования RAM когда выполняется цикл, видимо всё-таки из-за кода в функции выше:\n",
    "\n",
    "```python\n",
    "text = ' '.join([str(el) for el in flatten_dict_into_str(doc['text'])])\n",
    "sentences = sent_tokenizer.tokenize(text)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = reader.docs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Берём сколько надо документов. Здесь данные, чтобы взять весь корпус (293 документа)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_doc = 293\n",
    "results = get_data_from_x_docs(docs, count_doc)\n",
    "time = datetime.now.strftime('%H.%M.%S-%d-%m')\n",
    "with open('out_x{}-{}.json'.format(count_doc, time), 'w', encoding='utf-8') as f:\n",
    "    json.dump(docs, f, ensure_ascii=False, indent=4)\n",
    "# end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "time = datetime.now().strftime('%H.%M.%S-%d-%m')\n",
    "with open('out_x{}-{}.json'.format(count_doc, time), 'w', encoding='utf-8') as f:\n",
    "    json.dump(results, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Импорт из json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Все данные могут быть сериализованы в json, т.к. данные - список словарей и внутри всё списки/словари. Выше мы сохранили данные в json, а здесь может их считать при последующем запуске программы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('out_x293-19.05.58-25-12.json', 'r', encoding='utf-8') as f:\n",
    "    results = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сделать из списка словарей, каждый из которых описывает один документ, датафрейм. Каждой паре слов из словаря присвоить метаданные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_dict_to_dataframe(results):\n",
    "    ## пока не учитываем ['contexts'], собравшиеся неправильно\n",
    "    \n",
    "    author = results['author']\n",
    "    title = results['title']\n",
    "    entries_list = []\n",
    "    \n",
    "    for sentence_and_word_pairs_dict in results['sentences']:\n",
    "        sentence = sentence_and_word_pairs_dict['sentence'].replace('\\n', ' ')\n",
    "        word_pairs = sentence_and_word_pairs_dict['strange_pairs']\n",
    "        contexts_for_pairs = sentence_and_word_pairs_dict['contexts']\n",
    "        for prep_dict, second_word_dict in word_pairs:\n",
    "            entry = {'prep': prep_dict['word'], **second_word_dict,\n",
    "                     'sentence': sentence,\n",
    "                     'author': author, 'title': title}\n",
    "            entries_list.append(entry)\n",
    "    \n",
    "    \n",
    "    df = pd.DataFrame(entries_list)\n",
    "    cols = ['prep', 'word', 'lemma', 'pos', 'person', 'number', 'tense', 'mood',\n",
    "            'voice', 'gender', 'case', 'degree', 'sentence', 'author', 'title']\n",
    "    df = df[cols]  \n",
    "    \n",
    "    df.replace('N/A', np.nan, inplace=True)\n",
    "    \n",
    "    dtypes = {cat_name: CategoricalDtype(categories=cat_map.values())\n",
    "              for cat_name, cat_map in categories.items()}\n",
    "    dtypes['prep'] = 'category'\n",
    "    \n",
    "    df_new = df.astype(dtypes)\n",
    "    \n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full = pd.concat(list(map(convert_dict_to_dataframe, results)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prep</th>\n",
       "      <th>word</th>\n",
       "      <th>lemma</th>\n",
       "      <th>pos</th>\n",
       "      <th>person</th>\n",
       "      <th>number</th>\n",
       "      <th>tense</th>\n",
       "      <th>mood</th>\n",
       "      <th>voice</th>\n",
       "      <th>gender</th>\n",
       "      <th>case</th>\n",
       "      <th>degree</th>\n",
       "      <th>sentence</th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>12460</td>\n",
       "      <td>12460</td>\n",
       "      <td>12460</td>\n",
       "      <td>12129</td>\n",
       "      <td>578</td>\n",
       "      <td>9253</td>\n",
       "      <td>781</td>\n",
       "      <td>779</td>\n",
       "      <td>781</td>\n",
       "      <td>8665</td>\n",
       "      <td>8661</td>\n",
       "      <td>26</td>\n",
       "      <td>12460</td>\n",
       "      <td>12460</td>\n",
       "      <td>12460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>9</td>\n",
       "      <td>1556</td>\n",
       "      <td>1008</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>11617</td>\n",
       "      <td>43</td>\n",
       "      <td>270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>a</td>\n",
       "      <td>me</td>\n",
       "      <td>punc</td>\n",
       "      <td>pronoun</td>\n",
       "      <td>third person</td>\n",
       "      <td>singular</td>\n",
       "      <td>present</td>\n",
       "      <td>indicative</td>\n",
       "      <td>active</td>\n",
       "      <td>masculine</td>\n",
       "      <td>accusative</td>\n",
       "      <td>superlative</td>\n",
       "      <td>pro di immortales!</td>\n",
       "      <td>cicero</td>\n",
       "      <td>Naturalis Historia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>2612</td>\n",
       "      <td>937</td>\n",
       "      <td>1244</td>\n",
       "      <td>3662</td>\n",
       "      <td>304</td>\n",
       "      <td>6915</td>\n",
       "      <td>448</td>\n",
       "      <td>238</td>\n",
       "      <td>612</td>\n",
       "      <td>4513</td>\n",
       "      <td>3412</td>\n",
       "      <td>13</td>\n",
       "      <td>8</td>\n",
       "      <td>4125</td>\n",
       "      <td>2341</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         prep   word  lemma      pos        person    number    tense  \\\n",
       "count   12460  12460  12460    12129           578      9253      781   \n",
       "unique      9   1556   1008       11             3         2        6   \n",
       "top         a     me   punc  pronoun  third person  singular  present   \n",
       "freq     2612    937   1244     3662           304      6915      448   \n",
       "\n",
       "              mood   voice     gender        case       degree  \\\n",
       "count          779     781       8665        8661           26   \n",
       "unique           7       2          3           6            2   \n",
       "top     indicative  active  masculine  accusative  superlative   \n",
       "freq           238     612       4513        3412           13   \n",
       "\n",
       "                  sentence  author               title  \n",
       "count                12460   12460               12460  \n",
       "unique               11617      43                 270  \n",
       "top     pro di immortales!  cicero  Naturalis Historia  \n",
       "freq                     8    4125                2341  "
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_full.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full[(pd.isnull(df_full['pos']))].groupby('word',as_index=False).size()#[['prep', 'word', 'lemma', 'pos', 'sentence']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_x = df_full[pd.notnull(df_full['pos'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prep</th>\n",
       "      <th>word</th>\n",
       "      <th>lemma</th>\n",
       "      <th>pos</th>\n",
       "      <th>person</th>\n",
       "      <th>number</th>\n",
       "      <th>tense</th>\n",
       "      <th>mood</th>\n",
       "      <th>voice</th>\n",
       "      <th>gender</th>\n",
       "      <th>case</th>\n",
       "      <th>degree</th>\n",
       "      <th>sentence</th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>12129</td>\n",
       "      <td>12129</td>\n",
       "      <td>12129</td>\n",
       "      <td>12129</td>\n",
       "      <td>578</td>\n",
       "      <td>9253</td>\n",
       "      <td>781</td>\n",
       "      <td>779</td>\n",
       "      <td>781</td>\n",
       "      <td>8665</td>\n",
       "      <td>8661</td>\n",
       "      <td>26</td>\n",
       "      <td>12129</td>\n",
       "      <td>12129</td>\n",
       "      <td>12129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>9</td>\n",
       "      <td>1543</td>\n",
       "      <td>999</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>11339</td>\n",
       "      <td>43</td>\n",
       "      <td>270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>a</td>\n",
       "      <td>me</td>\n",
       "      <td>punc</td>\n",
       "      <td>pronoun</td>\n",
       "      <td>third person</td>\n",
       "      <td>singular</td>\n",
       "      <td>present</td>\n",
       "      <td>indicative</td>\n",
       "      <td>active</td>\n",
       "      <td>masculine</td>\n",
       "      <td>accusative</td>\n",
       "      <td>superlative</td>\n",
       "      <td>pro di immortales!</td>\n",
       "      <td>cicero</td>\n",
       "      <td>Naturalis Historia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>2514</td>\n",
       "      <td>937</td>\n",
       "      <td>1244</td>\n",
       "      <td>3662</td>\n",
       "      <td>304</td>\n",
       "      <td>6915</td>\n",
       "      <td>448</td>\n",
       "      <td>238</td>\n",
       "      <td>612</td>\n",
       "      <td>4513</td>\n",
       "      <td>3412</td>\n",
       "      <td>13</td>\n",
       "      <td>8</td>\n",
       "      <td>4111</td>\n",
       "      <td>2062</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         prep   word  lemma      pos        person    number    tense  \\\n",
       "count   12129  12129  12129    12129           578      9253      781   \n",
       "unique      9   1543    999       11             3         2        6   \n",
       "top         a     me   punc  pronoun  third person  singular  present   \n",
       "freq     2514    937   1244     3662           304      6915      448   \n",
       "\n",
       "              mood   voice     gender        case       degree  \\\n",
       "count          779     781       8665        8661           26   \n",
       "unique           7       2          3           6            2   \n",
       "top     indicative  active  masculine  accusative  superlative   \n",
       "freq           238     612       4513        3412           13   \n",
       "\n",
       "                  sentence  author               title  \n",
       "count                12129   12129               12129  \n",
       "unique               11339      43                 270  \n",
       "top     pro di immortales!  cicero  Naturalis Historia  \n",
       "freq                     8    4111                2062  "
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_x.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df_full[['pos', 'word', 'lemma', 'sentence']].groupby(['pos', 'word'])['word'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "for pos in pos_general['pos']:\n",
    "    example_words = df_x[df_x['pos'] == pos].groupby('word')['lemma'].count()\n",
    "    print('pos is', pos, '\\n', \n",
    "          example_words, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df_x[['prep', 'pos', 'word']].groupby(['prep', 'pos'])['word'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df_x[df_x['word'] == '%END%']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df_x[df_x['pos'] == 'preposition'][['prep','word', 'sentence']].groupby(\n",
    "    ['prep', 'word', 'sentence']\n",
    ")['word'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "# Сделать статистику для:\n",
    "\n",
    "* Часть речи\n",
    "    > Предлог -> часть речи и наоборот\n",
    "    \n",
    "* Для частей речи склоняющихся по падежу:\n",
    "    > Предлог -> Падеж (наоборот тоже?)\n",
    "    \n",
    "    Все вместе, и по каждой в отдельности\n",
    "    \n",
    "    \n",
    "**ЕСТЬ**:\n",
    "* Части речи\n",
    "* Предлог -> падеж (в целом)\n",
    "* Часть речи -> падеж (для каждой части речи)\n",
    "* Несклоняемая часть речи -> x частых лемм\n",
    "* Автор"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Статистика по собранным данным"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```len(results)``` -  276 (а всего-то 293!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Количество по части речи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_general = df_x.groupby(['pos'], as_index=False)['word'].count()\n",
    "pos_general_sorted = pos_general.sort_values(['word'], ascending=False).reset_index(drop=True)\n",
    "pos_general_sorted_nonull = pos_general_sorted[pos_general_sorted['word'] != 0]\n",
    "pos_general"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(18, 14))\n",
    "sns_plot = sns.barplot(x='pos', y='word', data=pos_general_sorted_nonull, order=pos_general_sorted_nonull['pos'].to_list())\n",
    "# sns_plot = sns.countplot(x='pos', data=df_x) - то же самое\n",
    "sns_plot.set(yscale=\"log\")\n",
    "plt.ylabel('Count')\n",
    "for bar in sns_plot.patches:\n",
    "        height = bar.get_height()\n",
    "        if not np.isnan(height):\n",
    "            height = int(height)\n",
    "        if height == 0:\n",
    "            continue\n",
    "\n",
    "        ax.text(bar.get_x() + bar.get_width() / 2, height + 5, f'{height}',\n",
    "                ha='center', va='bottom')\n",
    "        \n",
    "print(sns_plot)\n",
    "plt.savefig('graphics/boxplot_posCount.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Количество по авторам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_by_author(author='all', most_common=5, plot_what='Count'):\n",
    "    df = df_x\n",
    "    total = len(df[pd.notnull(df['author'])])\n",
    "\n",
    "    if author != 'all':\n",
    "        df = df_x[df_x['author'] == 'author']\n",
    "        \n",
    "    author_general = df.groupby('author', as_index=False).size()\n",
    "    author_general_sorted = author_general.sort_values(ascending=False).reset_index()\n",
    "    author_ratio = author_general_sorted.assign(ratio=lambda x: x[0] / total)\n",
    "    author_ratio.astype({0: int})\n",
    "    \n",
    "    print('\\n', author_ratio)\n",
    "    print(author_general_sorted.index, author_general_sorted.columns)\n",
    "    \n",
    "    f, ax = plt.subplots(figsize=(18, 14))\n",
    "    if plot_what == 'Count':\n",
    "        author_plot = sns.barplot(x='author', y=0, data=author_ratio.loc[0:most_common])\n",
    "    elif plot_what == 'Ratio':\n",
    "        author_ratio = author_ratio.sort_values('ratio', ascending=False)\n",
    "        author_plot = sns.barplot(x='author', y='ratio', data=author_ratio.loc[0:most_common])\n",
    "    else:\n",
    "        raise('wrong input')\n",
    "    \n",
    "    plt.xticks(rotation=25)\n",
    "    plt.ylabel(f'{plot_what}')\n",
    "    plt.title(f'{plot_what} of entries by author')\n",
    "    \n",
    "    for bar in author_plot.patches:\n",
    "            height = bar.get_height()\n",
    "            if not np.isnan(height) and not height is int:\n",
    "                height = round(float(height), 2)\n",
    "            elif height is int:\n",
    "                height = int(height)\n",
    "            if height == 0:\n",
    "                continue\n",
    "\n",
    "            ax.annotate(f'{height}',\n",
    "                    xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                    xytext=(0, 3),  # 3 points vertical offset\n",
    "                    textcoords=\"offset points\", ha='center', va='bottom')\n",
    "\n",
    "    print(author_plot)\n",
    "    plt.savefig(f'graphics/boxplot_author{plot_what}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_by_author()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Количество по падежу по части речи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def groupby_and_plot_by_pars(filter_col, filter_col_values, group_on, aggr_on, df=df_x, plot_null=False, want_plot=True):\n",
    "    if not filter_col or not filter_col_values:\n",
    "        df_fragment = df\n",
    "    else:\n",
    "        df_fragment = df[df[filter_col] == filter_col_values]\n",
    "        \n",
    "    grouped_df_for_col = df_fragment.groupby(group_on, as_index=False)[aggr_on].count()\n",
    "    \n",
    "    sorted_groups = grouped_df_for_col.sort_values([aggr_on], ascending=False).reset_index(drop=True)\n",
    "    if not plot_null:\n",
    "        sorted_groups = sorted_groups[sorted_groups[aggr_on] != 0]\n",
    "    print(f'Groupping df_x where \"{filter_col}\" only has values: \"{filter_col_values}\". '\n",
    "          f'Groupping by \"{group_on}\". (aggregation by \"{aggr_on}\")\\n\\n', sorted_groups, '\\n')\n",
    "    \n",
    "    if not want_plot:\n",
    "        return\n",
    "    \n",
    "    ### сохранять цвет\n",
    "#     if group_on == 'case':\n",
    "#         colors = {'nominative': 'C0'\n",
    "#                     'genitive': 'C1'\n",
    "#                     'dative': 'C2'\n",
    "#                     'accusative': 'C3'\n",
    "#                     'vocative': 'C4'\n",
    "#                     'locative': 'C5'}\n",
    "\n",
    "    \n",
    "    f, ax = plt.subplots(figsize=(18, 14))\n",
    "    groupon_values = sns.barplot(x=group_on, y=aggr_on, \n",
    "                                 data=sorted_groups, order=sorted_groups[group_on].to_list())\n",
    "    groupon_values.set(yscale=\"log\")\n",
    "    plt.ylabel('Count')\n",
    "    plt.title(f'Number of {filter_col_values}s by {group_on}')\n",
    "    \n",
    "    for bar in groupon_values.patches:\n",
    "            height = bar.get_height()\n",
    "            if not np.isnan(height):\n",
    "                height = int(height)\n",
    "            if height == 0:\n",
    "                continue\n",
    "\n",
    "            ax.annotate(f'{height}',\n",
    "                    xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                    xytext=(0, 3),  # 3 points vertical offset\n",
    "                    textcoords=\"offset points\", ha='center', va='bottom')\n",
    "\n",
    "    print(groupon_values)\n",
    "    plt.savefig(f'graphics/boxplot_{filter_col}={filter_col_values}_{group_on}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_with_case = ['pronoun', 'noun', 'adjective', 'participle']\n",
    "for pos in pos_with_case:\n",
    "    groupby_and_plot_by_pars('pos', pos, 'case', 'word', plot_null=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Настаканный график](https://stackoverflow.com/questions/26683654/making-a-stacked-barchart-in-pandas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Количество падежей по предлогу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def case_by_prep(prep='all'):\n",
    "    \n",
    "    df = df_x\n",
    "    if not prep == 'all':\n",
    "        df = df_x[df_x['prep'].isin(prep)]\n",
    "    \n",
    "    df_for_case = df[pd.notnull(df['case'])][['prep', 'case', 'word']]\n",
    "\n",
    "    case_by_prep_groupby = df_for_case.groupby(['prep', 'case'], as_index=False).size()\n",
    "    sorted_case_by_prep_groupby = case_by_prep_groupby.sort_values(ascending=False)\n",
    "    df_case_by_prep = sorted_case_by_prep_groupby.astype('int').unstack()\n",
    "    df_case_by_prep\n",
    "\n",
    "    sum_by_prep = df_case_by_prep.sum(axis=1).reset_index()\n",
    "    sorted_sum_by_prep = sum_by_prep.sort_values(0, ascending=False)\n",
    "\n",
    "    ordered_case_by_prep = df_case_by_prep.reindex(sorted_sum_by_prep['prep'].to_list())\n",
    "    print(ordered_case_by_prep)\n",
    "    \n",
    "    figsize=(12,12)\n",
    "    if not prep == 'all':\n",
    "        figsize=(12,12)\n",
    "        \n",
    "    f, ax = plt.subplots(figsize=figsize)\n",
    "    case_by_prep = ordered_case_by_prep.plot(ax=ax, kind='bar', stacked=True)\n",
    "    plt.ylabel('Count')\n",
    "    plt.title(f'Number of cases for preposition: {prep}')\n",
    "    ### возможно добавить подписи?\n",
    "    \n",
    "    print(case_by_prep)\n",
    "    plt.savefig(f'graphics/boxplot_caseBy_prep={prep}.png')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case_by_prep(['cum'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Примеры для склоняемых частей речи после предлогов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info(count=10,\n",
    "             pos=df_x['pos'].to_list(),\n",
    "             case=['nominative', 'genitive', 'dative', 'accusative', 'locative', 'vocative'], \n",
    "             prep=['a', 'ab', 'de', 'cum', 'ex', 'e', 'sine', 'pro', 'prae']):\n",
    "    \n",
    "            \n",
    "    df_by_case = df_x[(df_x['case'].isin(case)) & (df_x['prep'].isin(prep)) & (df_x['pos'].isin(pos))][['prep', 'word', 'pos', 'case', 'number', 'sentence']]\n",
    "    sentence, prep, word, case_, num_, pos_ =   (df_by_case['sentence'].to_list(), df_by_case['prep'].to_list(), \n",
    "                                                 df_by_case['word'].to_list(), df_by_case['case'].to_list(),\n",
    "                                                 df_by_case['number'].to_list(), df_by_case['pos'].to_list())\n",
    "\n",
    "    \n",
    "    a = list(zip(sentence, prep, word, case_, num_, pos_))\n",
    "    examples = [random.choice(a) for i in range(count)]\n",
    "    \n",
    "    return len(a), examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cur igitur mundus non animans sapiensque iudicetur, cum ex se procreet animantis atque sapientis?\"',\n",
       "  'ex',\n",
       "  'se',\n",
       "  'accusative',\n",
       "  'singular',\n",
       "  'pronoun'),\n",
       " ('id quoque a me impetrat.', 'a', 'me', 'accusative', 'singular', 'pronoun'),\n",
       " ('sunt aliae naturales quibusdam praeter- que vernam , quae suis constant sideribus—quorum ratio aptius reddetur tertio ab hoc volumine—, hiberna  aquilae exortu, aestiva canis ortu, tertia arcturi.',\n",
       "  'ab',\n",
       "  'hoc',\n",
       "  'accusative',\n",
       "  'singular',\n",
       "  'pronoun'),\n",
       " ('Non curo istunc, de illa quaero.',\n",
       "  'de',\n",
       "  'illa',\n",
       "  'accusative',\n",
       "  'plural',\n",
       "  'pronoun'),\n",
       " ('nam multo propius accedere ad scriptoris voluntatem eum, qui ex ipsius eam litteris interpretetur, quam illum, qui sententiam scriptoris non ex ipsius scripto spectet, quod ille suae voluntatis quasi imaginem reliquerit, sed domesticis suspicionibus perscrutetur.',\n",
       "  'ex',\n",
       "  'ipsius',\n",
       "  'genitive',\n",
       "  'singular',\n",
       "  'pronoun'),\n",
       " ('dissuasimus nos, sed nihil de me, de Scipione dicam libentius.',\n",
       "  'de',\n",
       "  'me',\n",
       "  'accusative',\n",
       "  'singular',\n",
       "  'pronoun'),\n",
       " ('reliquum est ut, cum cognorim pluribus rebus quid tu et de bonorum fortuna et de rei p. calamitatibus sentires, nihil a te petam nisi ut ad eam voluntatem, quam tua sponte erga Caecinam habiturus es, tantus cumulus accedat commendatione mea, quanti me a te fieri intellego.',\n",
       "  'a',\n",
       "  'te',\n",
       "  'accusative',\n",
       "  'singular',\n",
       "  'pronoun'),\n",
       " ('Quae cum ita sint, paululum equidem de me deprecabor et petam a vobis, ut ea, quae dicam, non de memet ipso, sed de oratore dicere putetis.',\n",
       "  'de',\n",
       "  'me',\n",
       "  'accusative',\n",
       "  'singular',\n",
       "  'pronoun'),\n",
       " ('Ille gravem medios silicem iaculatus in hostes a se depulsum Martem convertit in ipsos.',\n",
       "  'a',\n",
       "  'se',\n",
       "  'accusative',\n",
       "  'singular',\n",
       "  'pronoun'),\n",
       " ('Si vero et factis aliquid tale pro peccatoribus edidit dominus, ut cum peccatrici feminae etiam corporis  sui contactum permittit lavanti lacrimis pedes eius et crinibus detergenti  tergenti et unguento sepulturam ipsius inauguranti, ut cum Samaritanae  sexto iam matrimonio non moechae, sed prostitutae, etiam    quod nemini facile, quis esset ostendit, nihil ex hoc adversariis confertur, etsi iam Christianis veniam delictorum praestitisset.',\n",
       "  'ex',\n",
       "  'hoc',\n",
       "  'accusative',\n",
       "  'singular',\n",
       "  'pronoun')]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_info(count=10, pos=['pronoun'])[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Предлог cum\n",
    "\n",
    "![boxplot_caseBy_prep=['cum'].png](graphics/boxplot_caseBy_prep=['cum'].png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_by_undeclinable_lemma(pos, count_by='lemma', most_common=7):\n",
    "    # count_by для совместимости с пунктуацией, у которой одна лемма...\n",
    "    \n",
    "    df = df_x[df_x['pos'] == pos]\n",
    "        \n",
    "    lemma_general = df.groupby(count_by, as_index=False).size()\n",
    "    lemma_general_sorted = lemma_general.sort_values(ascending=False).reset_index()\n",
    "    print(lemma_general_sorted)\n",
    "    \n",
    "    f, ax = plt.subplots(figsize=(18, 14))\n",
    "    lemma_plot = sns.barplot(x=count_by, y=0, data=lemma_general_sorted.loc[0:most_common])\n",
    "    plt.xticks(rotation=25)\n",
    "    plt.ylabel('Count')\n",
    "    plt.title(f'Count of entries by lemma for pos: {pos}')\n",
    "    \n",
    "    for bar in lemma_plot.patches:\n",
    "            height = bar.get_height()\n",
    "            if not np.isnan(height):\n",
    "                height = int(height)\n",
    "            if height == 0:\n",
    "                continue\n",
    "\n",
    "            ax.annotate(f'{height}',\n",
    "                    xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                    xytext=(0, 3),  # 3 points vertical offset\n",
    "                    textcoords=\"offset points\", ha='center', va='bottom')\n",
    "\n",
    "    print(lemma_plot)\n",
    "    plt.savefig(f'graphics/boxplot_lemmaCount_pos={pos}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_lemma = ['numeral', 'conjunction', 'exclamation', 'preposition', 'adverb']\n",
    "no_lemma = ['punctuation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pos in has_lemma:\n",
    "    count_by_undeclinable_lemma(pos, most_common=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NB** ex eo. Отфильтровать"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_by_undeclinable_lemma(no_lemma[0], count_by='word', most_common=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info_und(count,\n",
    "             pos=df_x['pos'].to_list(),\n",
    "             prep=['a', 'ab', 'de', 'cum', 'ex', 'e', 'sine', 'pro', 'prae']):\n",
    "        \n",
    "    df_by_case = df_x[(df_x['prep'].isin(prep)) & (df_x['pos'].isin(pos))][['prep', 'word', 'pos', 'sentence']]\n",
    "    sentence, prep, word, pos_ =   (df_by_case['sentence'].to_list(), df_by_case['prep'].to_list(), \n",
    "                                                 df_by_case['word'].to_list(), df_by_case['pos'].to_list())\n",
    "    \n",
    "    a = list(zip(prep, word, pos_, sentence))\n",
    "    examples = [random.choice(a) for i in range(count)]\n",
    "    \n",
    "    return len(a), examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ex',\n",
       "  '-',\n",
       "  'punctuation',\n",
       "  'tremulis, spasticis, ex- ilientibus et quibus cor palpitet aliquid ex corde coctum mandendum ita, ut reliquae partis cinis cum cerebro hyaenae inlinatur;'),\n",
       " ('e', '.', 'punctuation', 'S. v. v. b. e. e. q. v. Cum pr.'),\n",
       " ('prae',\n",
       "  '-',\n",
       "  'punctuation',\n",
       "  'lineas ex argento nigras prae- duci plerique mirantur .'),\n",
       " ('a',\n",
       "  '.',\n",
       "  'punctuation',\n",
       "  'scripta epistula litterae mihi ante lucem a Lepta  Capua redditae sunt Idib. Mart. Pompeium a  Brundisio conscendisse, at Caesarem a. d. vii Kal.  Aprilis   Capuae fore.'),\n",
       " ('prae',\n",
       "  '-',\n",
       "  'punctuation',\n",
       "  'Arabicae excellunt candore, circulo prae- lucido atque non gracili neque in recessu gemmae aut in deiectu renidente , sed in ipsis umbonibus nitente, prae-     terea substrato nigerrimi coloris .'),\n",
       " ('de',\n",
       "  '-',\n",
       "  'punctuation',\n",
       "  'sic et morbo regio et hydropicis prodesse, etiam in choleris de- stillationes stomachi inhiberi.'),\n",
       " ('e',\n",
       "  ',',\n",
       "  'punctuation',\n",
       "  'fica vi appellatum ter die fieri amarum salsumque ac deinde dulcem totiensque et noct e , scatentem albis ser- pentibus vicenum cubitorum;'),\n",
       " ('pro',\n",
       "  '-',\n",
       "  'punctuation',\n",
       "  'Asia picem Idaeam maxime pro- bat, Graecia Piericam, Vergilius Naryciam.'),\n",
       " ('e',\n",
       "  ',',\n",
       "  'punctuation',\n",
       "  'Mago ante annum iubet, ut solem pluviasque conbibant, aut, si id condicio largita non sit, ignes in mediis fieri ante menses duos, nec nisi post im- bres in his seri, altitudinem eorum in argilloso aut duro solo trium cubitorum esse in quamque partem, in pr o nis  palmo amplius, iubetque caminata fossura ore conpressiore  e ss e , in nigra vero terra duo cubita et palmum quadratis angulis eadem mensura.'),\n",
       " ('de',\n",
       "  '-',\n",
       "  'punctuation',\n",
       "  'quo- niam in umido solo ad vicina aquae perveniat ur , Cato,     si locus aquosus sit, latos pedes ternos in faucibus imos- que palmum et pedem, altitudine IIII pedum, eos lapide consterni aut, si non sit, perticis salignis viridibus, si ne- que hae sint, sarmentis, ita ut in altitudine m semipes de- trahatur .')]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_info_und(10, pos=['punctuation'])[1]"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-",
   "formats": "ipynb,py:hydrogen"
  },
  "kernelspec": {
   "display_name": "Python (cltk)",
   "language": "python",
   "name": "cltk_project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
