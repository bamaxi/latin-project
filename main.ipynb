{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cltk.corpus.utils.importer import CorpusImporter\n",
    "from cltk.corpus.readers import get_corpus_reader\n",
    "from cltk.tag.pos import POSTag\n",
    "from cltk.lemmatize.latin.backoff import BackoffLatinLemmatizer\n",
    "from cltk.tokenize.latin.sentence import SentenceTokenizer\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_importer = CorpusImporter('latin')\n",
    "\n",
    "corpus_importer.import_corpus('latin_models_cltk')\n",
    "\n",
    "corpus_importer.import_corpus('latin_text_perseus')\n",
    "\n",
    "reader = get_corpus_reader(language='latin', corpus_name='latin_text_perseus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = list(reader.docs())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perseus_sents = list(reader.sents())\n",
    "\n",
    "len(perseus_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('out.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(docs, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger = POSTag('latin')\n",
    "lemmatizer = BackoffLatinLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_tokenizer = SentenceTokenizer(strict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Gallia', None),\n",
       " ('est', 'V3SPIA---'),\n",
       " ('omnis', 'A-S---MN-'),\n",
       " ('divisa', 'T-PRPPNN-'),\n",
       " ('in', 'R--------'),\n",
       " ('partes', 'N-P---FA-'),\n",
       " ('tres', 'M--------')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger.tag_ngram_123_backoff('Gallia est omnis divisa in partes tres')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция находит в предложении предлоги, употребляющиеся с аблативом, и стоящие после них слова. Если есть что-то необычное, например, слово не в аблативе, или не имеющее падежа, то оно возвращается.\n",
    "\n",
    "1: \tpart of speech\n",
    " \n",
    " \tn\tnoun\n",
    " \tv\tverb\n",
    " \tt\tparticiple\n",
    " \ta\tadjective\n",
    " \td\tadverb\n",
    " \tc\tconjunction\n",
    " \tr\tpreposition\n",
    " \tp\tpronoun\n",
    " \tm\tnumeral\n",
    " \ti\tinterjection\n",
    " \te\texclamation\n",
    " \tu\tpunctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_analysis_to_dict(analysis, keep_empty=True):\n",
    "\n",
    "    pos = {\n",
    "        'n': 'noun',\n",
    "        'v': 'verb',\n",
    "        't': 'participle',\n",
    "        'a': 'adjective',\n",
    "        'd': 'adverb',\n",
    "        'c': 'conjunction',\n",
    "        'r': 'preposition',\n",
    "        'p': 'pronoun',\n",
    "        'm': 'numeral',\n",
    "        'i': 'interjection',\n",
    "        'e': 'exclamation',\n",
    "        'u': 'punctuation'\n",
    "    }\n",
    "    \n",
    "    person = {\n",
    "        '1': 'first person',\n",
    "        '2': 'second person',\n",
    "        '3': 'third person'\n",
    "    }\n",
    "    \n",
    "    number = {\n",
    "        's': 'singular',\n",
    "        'p': 'plural'\n",
    "    }\n",
    "    \n",
    "    tense = {\n",
    "        'p': 'present',\n",
    "        'i': 'imperfect',\n",
    "        'r': 'perfect',\n",
    "        'l': 'pluperfect',\n",
    "        't': 'future perfect',\n",
    "        'f': 'future',\n",
    "    }\n",
    "    \n",
    "    mood = {\n",
    "        'i': 'indicative',\n",
    "        's': 'subjunctive',\n",
    "        'n': 'infinitive',\n",
    "        'm': 'imperative',\n",
    "        'p': 'participle',\n",
    "        'd': 'gerund',\n",
    "        'g': 'gerundive',\n",
    "        'u': 'supine',\n",
    "    }\n",
    "    \n",
    "    voice = {\n",
    "        'a': 'active',\n",
    "        'p': 'passive',\n",
    "    }\n",
    "    \n",
    "    gender = {\n",
    "        'm': 'masculine',\n",
    "        'f': 'feminine',\n",
    "        'n': 'neuter',\n",
    "    }\n",
    "    \n",
    "    case = {\n",
    "        'n': 'nominative',\n",
    "        'g': 'genitive',\n",
    "        'd': 'dative',\n",
    "        'a': 'accusative',\n",
    "        'b': 'ablative',\n",
    "        'v': 'vocative',\n",
    "        'l': 'locative',\n",
    "    }\n",
    "    \n",
    "    degree = {\n",
    "        'c': 'comparative',\n",
    "        's': 'superlative',\n",
    "    }\n",
    "    \n",
    "    categories = {1: pos, 2: person, 3: number, 4: tense, 5:mood,\n",
    "                  6: voice, 7: gender, 8: case, 9: degree}\n",
    "    categories_names = {1: 'pos', 2: 'person', 3: 'number', 4: 'tense',\n",
    "                        5: 'mood', 6: 'voice', 7: 'gender', 8: 'case',\n",
    "                        9: 'degree'}\n",
    "    \n",
    "    \n",
    "    dict_analysis = {}\n",
    "    \n",
    "    for i, cat_value_letter in enumerate(analysis, start=1):\n",
    "        cat_name = categories_names[i]\n",
    "        if cat_value_letter == '-':\n",
    "            if keep_empty:\n",
    "                dict_analysis[cat_name] = 'N/A'\n",
    "            continue\n",
    "        try:    \n",
    "            cat_value_word = categories[i][cat_value_letter.lower()]\n",
    "            dict_analysis[cat_name] = cat_value_word\n",
    "        except KeyError as k_e:\n",
    "            print(k_e)\n",
    "            print(cat_value_letter, analysis)\n",
    "            \n",
    "    return dict_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_non_ablative_after_preposition(sent):\n",
    "    # pro вроде иногда с Acc\n",
    "    abl_preps = ('a', 'ab', 'de', 'cum', 'ex', 'e', 'sine', 'pro', 'prae')\n",
    "    results = {'sentence': None, 'strange_pairs': []}\n",
    "    \n",
    "    tagged_words =  tagger.tag_ngram_123_backoff(sent)\n",
    "    words_number = len(tagged_words)\n",
    "    \n",
    "    for i, (word, analysis) in enumerate(tagged_words):        \n",
    "        if analysis is None:# or analysis == 'None':\n",
    "            continue\n",
    "        \n",
    "        is_needed_prep = analysis[0] == 'R' and word in abl_preps\n",
    "        if not is_needed_prep:\n",
    "            continue\n",
    "        \n",
    "#         word_with_verbose_analysis = convert_analysis_to_dict(word, analysis)\n",
    "        verbose_analysis = convert_analysis_to_dict(analysis)\n",
    "        verbose_analysis['word'] = word\n",
    "        word_lemma = lemmatizer.lemmatize([word])[0][1]\n",
    "        verbose_analysis['lemma'] = word_lemma\n",
    "\n",
    "        is_last = i == words_number - 1        \n",
    "        if is_last:\n",
    "            results['sentence'] = sent\n",
    "            results['strange_pairs'].append(\n",
    "                (verbose_analysis, ('%END%', '%END')))\n",
    "        else:\n",
    "            next_word, next_word_analysis = tagged_words[i+1]\n",
    "            if next_word_analysis is None:\n",
    "                continue\n",
    "                \n",
    "            if next_word_analysis[8-1].lower() != 'b':\n",
    "                next_word_verbose_analysis = convert_analysis_to_dict(\n",
    "                    next_word_analysis)\n",
    "                next_word_verbose_analysis['word'] = next_word\n",
    "                next_word_lemma = lemmatizer.lemmatize([next_word])[0][1]\n",
    "                next_word_verbose_analysis['lemma'] = next_word_lemma\n",
    "                \n",
    "                results['sentence'] = sent\n",
    "                results['strange_pairs'].append((verbose_analysis,\n",
    "                                                next_word_verbose_analysis))\n",
    "            \n",
    "    if results['sentence'] is None:\n",
    "        return None\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_dict_into_str(iter_):\n",
    "    if isinstance(iter_, str):\n",
    "        yield iter_\n",
    "    else:\n",
    "        try:\n",
    "            for i, obj in iter_.items():\n",
    "                yield from flatten_dict_into_str(obj)\n",
    "        except:\n",
    "            try:\n",
    "                for obj in iter_:\n",
    "                    yield from flatten_dict_into_str(obj)\n",
    "            except:\n",
    "                yield iter_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_document(doc):\n",
    "    analysis_for_doc = {'author': doc['author'], 'title': doc['originalTitle'],\n",
    "                       'sentences': []}\n",
    "    \n",
    "    text = ' '.join(map(str, flatten_dict_into_str(doc['text'])))\n",
    "    \n",
    "    sentences = sent_tokenizer.tokenize(text)\n",
    "    for sent in tqdm(sentences):\n",
    "        sent_data = get_non_ablative_after_preposition(sent)\n",
    "        if sent_data is None:\n",
    "            continue\n",
    "            \n",
    "        analysis_for_doc['sentences'].append(sent_data)\n",
    "    \n",
    "    return analysis_for_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66f8cff453304166ab79fa09936c670b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7c2ad3701a34a659d76b431f0dfc7b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5006.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6194860054cd425aaf4c5d131ea6665e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1315.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'1'\n",
      "1 V1SPIA---\n",
      "'3'\n",
      "3 V3SPSA---\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# count = 3000\n",
    "\n",
    "# results = []\n",
    "\n",
    "\n",
    "# for sent in tqdm(perseus_sents[:count]):\n",
    "#     result = get_non_ablative_after_preposition(sent)\n",
    "#     if result is not None:\n",
    "#         results.append(result)\n",
    "\n",
    "count_doc = 2\n",
    "\n",
    "results = []\n",
    "\n",
    "for doc in tqdm(docs[:count_doc]):\n",
    "    result = analyse_document(doc)\n",
    "    if result is not None:\n",
    "        results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('res_2docs_perseus.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(results, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "184"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(results[0]['sentences'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сделать из списка словарей, каждый из которых описывает один документ, датафрейм. Каждой паре слов из словаря присвоить метаданные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_dict_to_dataframe(results):\n",
    "    author = results['author']\n",
    "    title = results['title']\n",
    "    entries_list = []\n",
    "    \n",
    "    for sentence_and_word_pairs_dict in results['sentences']:\n",
    "        sentence = sentence_and_word_pairs_dict['sentence'].replace('\\n', ' ')\n",
    "        word_pairs = sentence_and_word_pairs_dict['strange_pairs']\n",
    "        for prep_dict, second_word_dict in word_pairs:\n",
    "            entry = {'prep': prep_dict['word'], **second_word_dict,\n",
    "                     'sentence': sentence,\n",
    "                     'author': author, 'title': title}\n",
    "            entries_list.append(entry)\n",
    "    \n",
    "    return pd.DataFrame(entries_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = convert_dict_to_dataframe(results[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prep</th>\n",
       "      <th>pos</th>\n",
       "      <th>person</th>\n",
       "      <th>number</th>\n",
       "      <th>tense</th>\n",
       "      <th>mood</th>\n",
       "      <th>voice</th>\n",
       "      <th>gender</th>\n",
       "      <th>case</th>\n",
       "      <th>degree</th>\n",
       "      <th>word</th>\n",
       "      <th>lemma</th>\n",
       "      <th>sentence</th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>189</td>\n",
       "      <td>189</td>\n",
       "      <td>189</td>\n",
       "      <td>189</td>\n",
       "      <td>189</td>\n",
       "      <td>189</td>\n",
       "      <td>189</td>\n",
       "      <td>189</td>\n",
       "      <td>189</td>\n",
       "      <td>189</td>\n",
       "      <td>189</td>\n",
       "      <td>189</td>\n",
       "      <td>189</td>\n",
       "      <td>189</td>\n",
       "      <td>189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>101</td>\n",
       "      <td>89</td>\n",
       "      <td>184</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>ex</td>\n",
       "      <td>noun</td>\n",
       "      <td>N/A</td>\n",
       "      <td>singular</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>feminine</td>\n",
       "      <td>genitive</td>\n",
       "      <td>N/A</td>\n",
       "      <td>se</td>\n",
       "      <td>sui</td>\n",
       "      <td>Ergo postquam factus est imperator Zeno a fili...</td>\n",
       "      <td>ammianus marcellinus.</td>\n",
       "      <td>Rerum Gestarum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>36</td>\n",
       "      <td>73</td>\n",
       "      <td>189</td>\n",
       "      <td>124</td>\n",
       "      <td>176</td>\n",
       "      <td>176</td>\n",
       "      <td>176</td>\n",
       "      <td>73</td>\n",
       "      <td>54</td>\n",
       "      <td>189</td>\n",
       "      <td>13</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>189</td>\n",
       "      <td>189</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       prep   pos person    number tense mood voice    gender      case  \\\n",
       "count   189   189    189       189   189  189   189       189       189   \n",
       "unique    9     9      1         3     3    4     3         4         6   \n",
       "top      ex  noun    N/A  singular   N/A  N/A   N/A  feminine  genitive   \n",
       "freq     36    73    189       124   176  176   176        73        54   \n",
       "\n",
       "       degree word lemma                                           sentence  \\\n",
       "count     189  189   189                                                189   \n",
       "unique      1  101    89                                                184   \n",
       "top       N/A   se   sui  Ergo postquam factus est imperator Zeno a fili...   \n",
       "freq      189   13    15                                                  3   \n",
       "\n",
       "                       author           title  \n",
       "count                     189             189  \n",
       "unique                      1               1  \n",
       "top     ammianus marcellinus.  Rerum Gestarum  \n",
       "freq                      189             189  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-",
   "formats": "ipynb,py:hydrogen"
  },
  "kernelspec": {
   "display_name": "Python (cltk)",
   "language": "python",
   "name": "cltk_project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
